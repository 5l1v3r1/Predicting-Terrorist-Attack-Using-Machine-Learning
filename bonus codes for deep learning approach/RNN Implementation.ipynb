{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This function will be used in the `preprocess.py`, and will in turn generate `train file`, `test file` and `validation file`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Preparing for the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'gtd.txt'\n",
    "f = open(file, 'r')\n",
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Variable Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2id, poi2id = {}, {}\n",
    "train_user, train_time, train_lat, train_lon, train_loc = [], [], [], [], []\n",
    "valid_user, valid_time, valid_lat, valid_lon, valid_loc = [], [], [], [], []\n",
    "test_user, test_time, test_lat, test_lon, test_loc = [], [], [], [], []\n",
    "user_time, user_lat, user_lon, user_loc = [], [], [], []\n",
    "attack_threshold = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Select Eligible Users and Creat `user2id` Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next line is to obtain the user id.\n",
    "prev_user = int(lines[0].split('\\t')[0])\n",
    "attack_cnt = 0\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    # The next line is to convert the splited line into a list.\n",
    "    tokens = line.strip().split('\\t')\n",
    "    # The next line obtains the original user id of the current line.\n",
    "    user = int(tokens[0])\n",
    "    if user == prev_user:\n",
    "        attack_cnt += 1\n",
    "    # This branch is effective when the line represents the next user.\n",
    "    else:\n",
    "        # This is to create a map from original id to new id.\n",
    "        # Only considers users having more records than the threshold\n",
    "        if attack_cnt >= attack_threshold:\n",
    "            user2id[prev_user] = len(user2id)\n",
    "        # This is to re-initiate the prev_user and attack_cnt\n",
    "        prev_user = user\n",
    "        attack_cnt = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Create the lists: _user, _lat, _lon, _loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_user = int(lines[0].split('\\t')[0])\n",
    "for i, line in enumerate(lines):\n",
    "    tokens = line.strip().split('\\t')\n",
    "    user = user2id.get(int(tokens[0]))\n",
    "    # The next line is to get rid of the users\n",
    "    # who have less than 30 records\n",
    "    if user is None:\n",
    "        continue\n",
    "    \n",
    "    # Now, we will only deal with users with more than 30 records.\n",
    "    time = (datetime.strptime(tokens[1], \"%Y-%m-%d\") - datetime(1970, 1, 1)).days\n",
    "    lat, lon, location = tokens[2], tokens[3], tokens[4]\n",
    "    \n",
    "    # The next line creates the poi2id dictionary.\n",
    "    # It maps the existing location id to a new id.\n",
    "    # The new id is defined upon the order of the appearance.\n",
    "    # You can view it as simply rename location id.\n",
    "    if poi2id.get(location) is None:\n",
    "        poi2id[location] = len(poi2id)\n",
    "    loc = poi2id.get(location)\n",
    "    \n",
    "    # When the user is the previous one,\n",
    "    # Just add his attributes into lists accordingly.\n",
    "    # Note that our file is ordered by user id.\n",
    "    if user == prev_user:\n",
    "        user_time.insert(0, time)\n",
    "        user_lat.insert(0, lat)\n",
    "        user_lon.insert(0, lon)\n",
    "        user_loc.insert(0, loc)\n",
    "    # We will update train / valid / test lists nnce new user appears.\n",
    "    # To illustrate, each element in train_time is a list containing\n",
    "    # the first 70% time record of a user. Others are similarly defined.\n",
    "    # Note that (i == len(lines) - 1) is the corner case for the last one\n",
    "    # Or it won't be included in the train/test/valid lists.\n",
    "    if (user != prev_user) or (i == len(lines) - 1):\n",
    "        train_threshold = int(len(user_time) * 0.7)\n",
    "        valid_threshold = int(len(user_time) * 0.8)\n",
    "            \n",
    "        train_user.append(user)\n",
    "        train_time.append(user_time[:train_threshold])\n",
    "        train_lat.append(user_lat[:train_threshold])\n",
    "        train_lon.append(user_lon[:train_threshold])\n",
    "        train_loc.append(user_loc[:train_threshold])\n",
    "            \n",
    "        valid_user.append(user)\n",
    "        valid_time.append(user_time[train_threshold:valid_threshold])\n",
    "        valid_lat.append(user_lat[train_threshold:valid_threshold])\n",
    "        valid_lon.append(user_lon[train_threshold:valid_threshold])\n",
    "        valid_loc.append(user_loc[train_threshold:valid_threshold])\n",
    "            \n",
    "        test_user.append(user)\n",
    "        test_time.append(user_time[valid_threshold:])\n",
    "        test_lat.append(user_lat[valid_threshold:])\n",
    "        test_lon.append(user_lon[valid_threshold:])\n",
    "        test_loc.append(user_loc[valid_threshold:])\n",
    "\n",
    "        prev_user = user\n",
    "        user_time = [time]\n",
    "        user_lat = [lat]\n",
    "        user_lon = [lon]\n",
    "        user_loc = [loc]      \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Function Formulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source_file):\n",
    "    f = open(source_file, 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    user2id, poi2id = {}, {}\n",
    "    train_user, train_time, train_lat, train_lon, train_loc = [], [], [], [], []\n",
    "    valid_user, valid_time, valid_lat, valid_lon, valid_loc = [], [], [], [], []\n",
    "    test_user, test_time, test_lat, test_lon, test_loc = [], [], [], [], []\n",
    "    user_time, user_lat, user_lon, user_loc = [], [], [], []\n",
    "    attack_threshold = 30\n",
    "    \n",
    "    # The next line is to obtain the user id.\n",
    "    prev_user = int(lines[0].split('\\t')[0])\n",
    "    attack_cnt = 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # The next line is to convert the splited line into a list.\n",
    "        tokens = line.strip().split('\\t')\n",
    "        # The next line obtains the original user id of the current line.\n",
    "        user = int(tokens[0])\n",
    "        if user == prev_user:\n",
    "            attack_cnt += 1\n",
    "        # This branch is effective when the line represents the next user.\n",
    "        else:\n",
    "            # This is to create a map from original id to new id.\n",
    "            # Only considers users having more records than the threshold\n",
    "            if attack_cnt >= attack_threshold:\n",
    "                user2id[prev_user] = len(user2id)\n",
    "            # This is to re-initiate the prev_user and attack_cnt\n",
    "            prev_user = user\n",
    "            attack_cnt = 1 \n",
    "\n",
    "    prev_user = int(lines[0].split('\\t')[0])\n",
    "    for i, line in enumerate(lines):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        user = user2id.get(int(tokens[0]))\n",
    "        # The next line is to get rid of the users\n",
    "        # who have less than 30 records\n",
    "        if user is None:\n",
    "            continue\n",
    "\n",
    "        # Now, we will only deal with users with more than 30 records.\n",
    "        time = (datetime.strptime(tokens[1], \"%Y-%m-%d\") - datetime(1970, 1, 1)).days\n",
    "        lat, lon, location = float(tokens[2]), float(tokens[3]), float(tokens[4])\n",
    "\n",
    "        # The next line creates the poi2id dictionary.\n",
    "        # It maps the existing location id to a new id.\n",
    "        # The new id is defined upon the order of the appearance.\n",
    "        # You can view it as simply rename location id.\n",
    "        if poi2id.get(location) is None:\n",
    "            poi2id[location] = len(poi2id)\n",
    "        loc = poi2id.get(location)\n",
    "\n",
    "        # When the user is the previous one,\n",
    "        # Just add his attributes into lists accordingly.\n",
    "        # Note that our file is ordered by user id.\n",
    "        if user == prev_user:\n",
    "            user_time.insert(0, time)\n",
    "            user_lat.insert(0, float(lat))\n",
    "            user_lon.insert(0, float(lon))\n",
    "            user_loc.insert(0, float(loc))\n",
    "        # We will update train / valid / test lists nnce new user appears.\n",
    "        # To illustrate, each element in train_time is a list containing\n",
    "        # the first 70% time record of a user. Others are similarly defined.\n",
    "        # Note that (i == len(lines) - 1) is the corner case for the last one\n",
    "        # Or it won't be included in the train/test/valid lists.\n",
    "        if (user != prev_user) or (i == len(lines) - 1):\n",
    "            train_threshold = int(len(user_time) * 0.7)\n",
    "            valid_threshold = int(len(user_time) * 0.8)\n",
    "\n",
    "            train_user.append(user)\n",
    "            train_time.append(user_time[:train_threshold])\n",
    "            train_lat.append(user_lat[:train_threshold])\n",
    "            train_lon.append(user_lon[:train_threshold])\n",
    "            train_loc.append(user_loc[:train_threshold])\n",
    "\n",
    "            valid_user.append(user)\n",
    "            valid_time.append(user_time[train_threshold:valid_threshold])\n",
    "            valid_lat.append(user_lat[train_threshold:valid_threshold])\n",
    "            valid_lon.append(user_lon[train_threshold:valid_threshold])\n",
    "            valid_loc.append(user_loc[train_threshold:valid_threshold])\n",
    "\n",
    "            test_user.append(user)\n",
    "            test_time.append(user_time[valid_threshold:])\n",
    "            test_lat.append(user_lat[valid_threshold:])\n",
    "            test_lon.append(user_lon[valid_threshold:])\n",
    "            test_loc.append(user_loc[valid_threshold:])\n",
    "\n",
    "            prev_user = user\n",
    "            user_time = [time]\n",
    "            user_lat = [lat]\n",
    "            user_lon = [lon]\n",
    "            user_loc = [loc]      \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return len(user2id), poi2id, \\\n",
    "           train_user, train_time, train_lat, train_lon, train_loc, \\\n",
    "           valid_user, valid_time, valid_lat, valid_lon, valid_loc, \\\n",
    "           test_user, test_time, test_lat, test_lon, test_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftype = torch.cuda.FloatTensor\n",
    "ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = 'gtd.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "user_cnt, poi2id, \\\n",
    "train_user, train_time, train_lati, train_longi, train_loc, \\\n",
    "valid_user, valid_time, valid_lati, valid_longi, valid_loc, \\\n",
    "test_user, test_time, test_lati, test_longi, test_loc = data_loader.\\\n",
    "                                                        load_data(source_file)\n",
    "print('Data loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User /Location: {:d} / {:d}'.format(user_cnt, len(poi2id)))\n",
    "print('=======================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Define Hyperparameters for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 7\n",
    "ww = 30\n",
    "up_time = 120\n",
    "lw_time = 5   \n",
    "up_dist = 100   \n",
    "lw_dist = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "evaluate_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Define the ST-RNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STRNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STRNNModule, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_weight = Variable(torch.randn(user_cnt, dim), requires_grad=False).type(ftype)\n",
    "        self.h_0 = Variable(torch.randn(dim, 1), requires_grad=False).type(ftype)\n",
    "        self.location_weight = nn.Embedding(len(poi2id), dim)\n",
    "        self.perm_weight = nn.Embedding(user_cnt, dim)\n",
    "        \n",
    "        # Attributes\n",
    "        self.time_upper = nn.Parameter(torch.randn(dim, dim).type(ftype))\n",
    "        self.time_lower = nn.Parameter(torch.randn(dim, dim).type(ftype))\n",
    "        self.dist_upper = nn.Parameter(torch.randn(dim, dim).type(ftype))\n",
    "        self.dist_lower = nn.Parameter(torch.randn(dim, dim).type(ftype))\n",
    "        self.C = nn.Parameter(torch.randn(dim, dim).type(ftype))\n",
    "\n",
    "        # Modules\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Find the most closest value to w, w_cap(index)\n",
    "    def find_w_cap(self, times, i):\n",
    "        trg_t = times[i] - ww\n",
    "        tmp_t = times[i]\n",
    "        tmp_i = i - 1\n",
    "        for idx, t_w in enumerate(reversed(times[:i]), start=1):\n",
    "            if t_w.data.cpu().numpy() == trg_t.data.cpu().numpy():\n",
    "                return i-idx\n",
    "            elif t_w.data.cpu().numpy() > trg_t.data.cpu().numpy():\n",
    "                tmp_t = t_w\n",
    "                tmp_i = i-idx\n",
    "            elif t_w.data.cpu().numpy() < trg_t.data.cpu().numpy():\n",
    "                if trg_t.data.cpu().numpy() - t_w.data.cpu().numpy() \\\n",
    "                    < tmp_t.data.cpu().numpy() - trg_t.data.cpu().numpy():\n",
    "                    return i-idx\n",
    "                else:\n",
    "                    return tmp_i\n",
    "        return 0\n",
    "\n",
    "    def return_h_tw(self, times, latis, longis, locs, idx):\n",
    "        w_cap = self.find_w_cap(times, idx)\n",
    "        if w_cap is 0:\n",
    "            return self.h_0\n",
    "        else:\n",
    "            self.return_h_tw(times, latis, longis, locs, w_cap)\n",
    "\n",
    "        lati = latis[idx] - latis[w_cap:idx]\n",
    "        longi = longis[idx] - longis[w_cap:idx]\n",
    "        td = times[idx] - times[w_cap:idx]\n",
    "        ld = self.euclidean_dist(lati, longi)\n",
    "\n",
    "        data = ','.join(str(e) for e in td.data.cpu().numpy()) + '\\t'\n",
    "        f.write(data)\n",
    "        data = ','.join(str(e) for e in ld.data.cpu().numpy()) + '\\t'\n",
    "        f.write(data)\n",
    "        data = ','.join(str(e.data.cpu().numpy()[0]) for e in locs[w_cap:idx]) + '\\t'\n",
    "        f.write(data)\n",
    "        data = str(locs[idx].data.cpu().numpy()[0]) + '\\n'\n",
    "        f.write(data)\n",
    "\n",
    "    # get transition matrices by linear interpolation\n",
    "    def get_location_vector(self, td, ld, locs):\n",
    "        tud = up_time - td\n",
    "        tdd = td - lw_time\n",
    "        lud = up_dist - ld\n",
    "        ldd = ld - lw_dist\n",
    "        loc_vec = 0\n",
    "        for i in xrange(len(tud)):\n",
    "            Tt = torch.div(torch.mul(self.time_upper, tud[i]) + torch.mul(self.time_lower, tdd[i]),\n",
    "                            tud[i]+tdd[i])\n",
    "            Sl = torch.div(torch.mul(self.dist_upper, lud[i]) + torch.mul(self.dist_lower, ldd[i]),\n",
    "                            lud[i]+ldd[i])\n",
    "            loc_vec += torch.mm(Sl, torch.mm(Tt, torch.t(self.location_weight(locs[i]))))\n",
    "        return loc_vec\n",
    "\n",
    "    def euclidean_dist(self, x, y):\n",
    "        return torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2))\n",
    "\n",
    "    def forward(self, user, times, latis, longis, locs, step):\n",
    "        f.write(str(user.data.cpu().numpy()[0])+\"\\n\")\n",
    "        # positive sampling\n",
    "        pos_h = self.return_h_tw(times, latis, longis, locs, len(times)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Define the Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(user, time, lati, longi, loc, step):\n",
    "\n",
    "    user = Variable(torch.from_numpy(np.asarray([user]))).type(ltype)\n",
    "    time = Variable(torch.from_numpy(np.asarray(time))).type(ftype)\n",
    "    lati = Variable(torch.from_numpy(np.asarray(lati))).type(ftype)\n",
    "    longi = Variable(torch.from_numpy(np.asarray(longi))).type(ftype)\n",
    "    loc = Variable(torch.from_numpy(np.asarray(loc))).type(ltype)\n",
    "\n",
    "    rnn_output = strnn_model(user, time, lati, longi, loc, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Run the Model and Write into the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strnn_model = STRNNModule().cuda()\n",
    "\n",
    "print(\"Making train file...\")\n",
    "f = open(\"/prepro_train_%s.txt\"%lw_time, 'w')\n",
    "# Training\n",
    "train_batches = list(zip(train_time, train_lati, train_longi, train_loc))\n",
    "for j, train_batch in enumerate(tqdm.tqdm(train_batches, desc=\"train\")):\n",
    "    batch_time, batch_lati, batch_longi, batch_loc = train_batch\n",
    "    run(train_user[j], batch_time, batch_lati, batch_longi, batch_loc, step=1)\n",
    "f.close()\n",
    "\n",
    "print(\"Making valid file...\")\n",
    "f = open(\"/prepro_valid_%s.txt\"%lw_time, 'w')\n",
    "# Eavludating\n",
    "valid_batches = list(zip(valid_time, valid_lati, valid_longi, valid_loc))\n",
    "for j, valid_batch in enumerate(tqdm.tqdm(valid_batches, desc=\"valid\")):\n",
    "    batch_time, batch_lati, batch_longi, batch_loc = valid_batch\n",
    "    run(valid_user[j], batch_time, batch_lati, batch_longi, batch_loc, step=2)\n",
    "f.close()\n",
    "\n",
    "print(\"Making test file...\")\n",
    "f = open(\"/prepro_test_%s.txt\"%lw_time, 'w')\n",
    "# Testing\n",
    "test_batches = list(zip(test_time, test_lati, test_longi, test_loc))\n",
    "for j, test_batch in enumerate(tqdm.tqdm(test_batches, desc=\"test\")):\n",
    "    batch_time, batch_lati, batch_longi, batch_loc = test_batch\n",
    "    run(test_user[j], batch_time, batch_lati, batch_longi, batch_loc, step=3)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Treat Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this function will not only works on the train file, but also on the valid and test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_prepro(train, step):\n",
    "    train_f = open(train, 'r')\n",
    "    if step==1:\n",
    "        lines = train_f.readlines()\n",
    "    elif step==2:\n",
    "        lines = train_f.readlines()\n",
    "    elif step==3:\n",
    "        lines = train_f.readlines()\n",
    "\n",
    "    train_user = []\n",
    "    train_td = []\n",
    "    train_ld = []\n",
    "    train_loc = []\n",
    "    train_dst = []\n",
    "\n",
    "    user = 1\n",
    "    user_td = []\n",
    "    user_ld = []\n",
    "    user_loc = []\n",
    "    user_dst = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if len(tokens) < 3:\n",
    "            if user_td: \n",
    "                train_user.append(user)\n",
    "                train_td.append(user_td)\n",
    "                train_ld.append(user_ld)\n",
    "                train_loc.append(user_loc)\n",
    "                train_dst.append(user_dst)\n",
    "            user = int(tokens[0])\n",
    "            user_td = []\n",
    "            user_ld = []\n",
    "            user_loc = []\n",
    "            user_dst = []\n",
    "            continue\n",
    "        td = np.array([float(t) for t in tokens[0].split(',')])\n",
    "        ld = np.array([float(t) for t in tokens[1].split(',')])\n",
    "        loc = np.array([int(t) for t in tokens[2].split(',')])\n",
    "        dst = int(tokens[3])\n",
    "        user_td.append(td)\n",
    "        user_ld.append(ld)\n",
    "        user_loc.append(loc)\n",
    "        user_dst.append(dst)\n",
    "\n",
    "    if user_td: \n",
    "        train_user.append(user)\n",
    "        train_td.append(user_td)\n",
    "        train_ld.append(user_ld)\n",
    "        train_loc.append(user_loc)\n",
    "        train_dst.append(user_dst)\n",
    "\n",
    "    return train_user, train_td, train_ld, train_loc, train_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import data_loader\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "ftype = torch.cuda.FloatTensor\n",
    "ltype = torch.cuda.LongTensor\n",
    "\n",
    "# Data loading params\n",
    "train_file = \"prepro_train_50.txt\"\n",
    "valid_file = \"prepro_valid_50.txt\"\n",
    "test_file = \"prepro_test_50.txt\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "dim = 7    # dimensionality\n",
    "ww = 30  # winodw width (6h)\n",
    "up_time = 365  # day\n",
    "lw_time = 0.\n",
    "up_dist = 457.335   # km\n",
    "lw_dist = 0.\n",
    "reg_lambda = 0.1\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 2\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "evaluate_every = 1\n",
    "h_0 = Variable(torch.randn(dim, 1), requires_grad=False).type(ftype)\n",
    "\n",
    "user_cnt = 294 \n",
    "loc_cnt = 28442\n",
    "\n",
    "\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "# Data Preparation\n",
    "# ===========================================================\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_user, train_td, train_ld, train_loc, train_dst = data_loader.treat_prepro(train_file, step=1)\n",
    "valid_user, valid_td, valid_ld, valid_loc, valid_dst = data_loader.treat_prepro(valid_file, step=2)\n",
    "test_user, test_td, test_ld, test_loc, test_dst = data_loader.treat_prepro(test_file, step=3)\n",
    "\n",
    "print(\"User/Location: {:d}/{:d}\".format(user_cnt, loc_cnt))\n",
    "print(\"==================================================================================\")\n",
    "\n",
    "class STRNNCell(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(STRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) # C\n",
    "        self.weight_th_upper = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) # T\n",
    "        self.weight_th_lower = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) # T\n",
    "        self.weight_sh_upper = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) # S\n",
    "        self.weight_sh_lower = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) # S\n",
    "\n",
    "        self.location_weight = nn.Embedding(loc_cnt, hidden_size)\n",
    "        self.permanet_weight = nn.Embedding(user_cnt, hidden_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, td_upper, td_lower, ld_upper, ld_lower, loc, hx):\n",
    "        loc_len = len(loc)\n",
    "        Ttd = [((self.weight_th_upper*td_upper[i] + self.weight_th_lower*td_lower[i])\\\n",
    "                /(td_upper[i]+td_lower[i])) for i in xrange(loc_len)]\n",
    "        Sld = [((self.weight_sh_upper*ld_upper[i] + self.weight_sh_lower*ld_lower[i])\\\n",
    "                /(ld_upper[i]+ld_lower[i])) for i in xrange(loc_len)]\n",
    "\n",
    "        loc = self.location_weight(loc).view(-1,self.hidden_size,1)\n",
    "        loc_vec = torch.sum(torch.cat([torch.mm(Sld[i], torch.mm(Ttd[i], loc[i]))\\\n",
    "                .view(1,self.hidden_size,1) for i in xrange(loc_len)], dim=0), dim=0)\n",
    "        usr_vec = torch.mm(self.weight_ih, hx)\n",
    "        hx = loc_vec + usr_vec # hidden_size x 1\n",
    "        return self.sigmoid(hx)\n",
    "\n",
    "    def loss(self, user, td_upper, td_lower, ld_upper, ld_lower, loc, dst, hx):\n",
    "        h_tq = self.forward(td_upper, td_lower, ld_upper, ld_lower, loc, hx)\n",
    "        p_u = self.permanet_weight(user)\n",
    "        q_v = self.location_weight(dst)\n",
    "        output = torch.mm(q_v, (h_tq + torch.t(p_u)))\n",
    "\n",
    "        return torch.log(1+torch.exp(torch.neg(output)))\n",
    "\n",
    "    def validation(self, user, td_upper, td_lower, ld_upper, ld_lower, loc, dst, hx):\n",
    "        # error exist in distance (ld_upper, ld_lower)\n",
    "        h_tq = self.forward(td_upper, td_lower, ld_upper, ld_lower, loc, hx)\n",
    "        p_u = self.permanet_weight(user)\n",
    "        user_vector = h_tq + torch.t(p_u)\n",
    "        ret = torch.mm(self.location_weight.weight, user_vector).data.cpu().numpy()\n",
    "        return np.argsort(np.squeeze(-1*ret))\n",
    "\n",
    "###############################################################################################\n",
    "def parameters():\n",
    "    params = []\n",
    "    for model in [strnn_model]:\n",
    "        params += list(model.parameters())\n",
    "\n",
    "    return params\n",
    "\n",
    "def print_score(batches, step):\n",
    "    recall1 = 0.\n",
    "    recall5 = 0.\n",
    "    recall10 = 0.\n",
    "    recall100 = 0.\n",
    "    recall1000 = 0.\n",
    "    recall10000 = 0.\n",
    "    iter_cnt = 0\n",
    "\n",
    "    for batch in tqdm.tqdm(batches, desc=\"validation\"):\n",
    "        batch_user, batch_td, batch_ld, batch_loc, batch_dst = batch\n",
    "        if len(batch_loc) < 3:\n",
    "            continue\n",
    "        iter_cnt += 1\n",
    "        batch_o, target = run(batch_user, batch_td, batch_ld, batch_loc, batch_dst, step=step)\n",
    "\n",
    "        recall1 += target in batch_o[:1]\n",
    "        recall5 += target in batch_o[:5]\n",
    "        recall10 += target in batch_o[:10]\n",
    "        recall100 += target in batch_o[:100]\n",
    "        recall1000 += target in batch_o[:1000]\n",
    "        recall10000 += target in batch_o[:10000]\n",
    "\n",
    "    print(\"recall@1: \", recall1/iter_cnt)\n",
    "    print(\"recall@5: \", recall5/iter_cnt)\n",
    "    print(\"recall@10: \", recall10/iter_cnt)\n",
    "    print(\"recall@100: \", recall100/iter_cnt)\n",
    "    print(\"recall@1000: \", recall1000/iter_cnt)\n",
    "    print(\"recall@10000: \", recall10000/iter_cnt)\n",
    "\n",
    "###############################################################################################\n",
    "def run(user, td, ld, loc, dst, step):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    seqlen = len(td)\n",
    "    user = Variable(torch.from_numpy(np.asarray([user]))).type(ltype)\n",
    "\n",
    "    #neg_loc = Variable(torch.FloatTensor(1).uniform_(0, len(poi2pos)-1).long()).type(ltype)\n",
    "    #(neg_lati, neg_longi) = poi2pos.get(neg_loc.data.cpu().numpy()[0])\n",
    "    rnn_output = h_0\n",
    "    for idx in range(seqlen - 1):\n",
    "        print(idx, up_time, td[idx])\n",
    "        td_upper = Variable(torch.from_numpy(np.asarray(up_time-td[idx]))).type(ftype)\n",
    "        td_lower = Variable(torch.from_numpy(np.asarray(td[idx]-lw_time))).type(ftype)\n",
    "        ld_upper = Variable(torch.from_numpy(np.asarray(up_dist-ld[idx]))).type(ftype)\n",
    "        ld_lower = Variable(torch.from_numpy(np.asarray(ld[idx]-lw_dist))).type(ftype)\n",
    "        location = Variable(torch.from_numpy(np.asarray(loc[idx]))).type(ltype)\n",
    "        rnn_output = strnn_model(td_upper, td_lower, ld_upper, ld_lower, location, rnn_output)#, neg_lati, neg_longi, neg_loc, step)\n",
    "\n",
    "    td_upper = Variable(torch.from_numpy(np.asarray(up_time-td[-1]))).type(ftype)\n",
    "    td_lower = Variable(torch.from_numpy(np.asarray(td[-1]-lw_time))).type(ftype)\n",
    "    ld_upper = Variable(torch.from_numpy(np.asarray(up_dist-ld[-1]))).type(ftype)\n",
    "    ld_lower = Variable(torch.from_numpy(np.asarray(ld[-1]-lw_dist))).type(ftype)\n",
    "    location = Variable(torch.from_numpy(np.asarray(loc[-1]))).type(ltype)\n",
    "\n",
    "    if step > 1:\n",
    "        return strnn_model.validation(user, td_upper, td_lower, ld_upper, ld_lower, location, dst[-1], rnn_output), dst[-1]\n",
    "\n",
    "    destination = Variable(torch.from_numpy(np.asarray([dst[-1]]))).type(ltype)\n",
    "    J = strnn_model.loss(user, td_upper, td_lower, ld_upper, ld_lower, location, destination, rnn_output)#, neg_lati, neg_longi, neg_loc, step)\n",
    "\n",
    "    J.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return J.data.cpu().numpy()\n",
    "\n",
    "###############################################################################################\n",
    "strnn_model = STRNNCell(dim).cuda()\n",
    "optimizer = torch.optim.SGD(parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_lambda)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Training\n",
    "    total_loss = 0.\n",
    "    train_batches = list(zip(train_user, train_td, train_ld, train_loc, train_dst))\n",
    "    for j, train_batch in enumerate(tqdm.tqdm(train_batches, desc=\"train\")):\n",
    "        #inner_batches = data_loader.inner_iter(train_batch, batch_size)\n",
    "        #for k, inner_batch in inner_batches:\n",
    "        batch_user, batch_td, batch_ld, batch_loc, batch_dst = train_batch#inner_batch)\n",
    "        if len(batch_loc) < 3:\n",
    "            continue\n",
    "        total_loss += run(batch_user, batch_td, batch_ld, batch_loc, batch_dst, step=1)\n",
    "        #if (j+1) % 2000 == 0:\n",
    "        #    print(\"batch #{:d}: \".format(j+1)), \"batch_loss :\", total_loss/j, datetime.datetime.now()\n",
    "    # Evaluation\n",
    "    if (i + 1) % evaluate_every == 0:\n",
    "        print(\"==================================================================================\")\n",
    "        #print(\"Evaluation at epoch #{:d}: \".format(i+1)), total_loss/j, datetime.datetime.now()\n",
    "        valid_batches = list(zip(valid_user, valid_td, valid_ld, valid_loc, valid_dst))\n",
    "        print_score(valid_batches, step=2)\n",
    "\n",
    "# Testing\n",
    "print(\"Training End..\")\n",
    "print(\"==================================================================================\")\n",
    "print(\"Test: \")\n",
    "test_batches = list(zip(test_user, test_td, test_ld, test_loc, test_dst))\n",
    "print_score(test_batches, step=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Treat Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This function will be used in the `train_torch.py`, to generate `x_user`, `x_td`, `x_ld`, `x_loc`, `x_dst`, where x means train, test, or valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_prepro(train, step):\n",
    "    train_f = open(train, 'r')\n",
    "    # Need to change depending on threshold\n",
    "    if step==1:\n",
    "        lines = train_f.readlines()#[:86445] #659 #[:309931]\n",
    "    elif step==2:\n",
    "        lines = train_f.readlines()#[:13505]#[:309931]\n",
    "    elif step==3:\n",
    "        lines = train_f.readlines()#[:30622]#[:309931]\n",
    "\n",
    "    train_user = []\n",
    "    train_td = []\n",
    "    train_ld = []\n",
    "    train_loc = []\n",
    "    train_dst = []\n",
    "\n",
    "    user = 1\n",
    "    user_td = []\n",
    "    user_ld = []\n",
    "    user_loc = []\n",
    "    user_dst = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if len(tokens) < 3:\n",
    "            if user_td: \n",
    "                train_user.append(user)\n",
    "                train_td.append(user_td)\n",
    "                train_ld.append(user_ld)\n",
    "                train_loc.append(user_loc)\n",
    "                train_dst.append(user_dst)\n",
    "            user = int(tokens[0])\n",
    "            user_td = []\n",
    "            user_ld = []\n",
    "            user_loc = []\n",
    "            user_dst = []\n",
    "            continue\n",
    "        td = np.array([float(t) for t in tokens[0].split(',')])\n",
    "        ld = np.array([float(t) for t in tokens[1].split(',')])\n",
    "        loc = np.array([int(t) for t in tokens[2].split(',')])\n",
    "        dst = int(tokens[3])\n",
    "        user_td.append(td)\n",
    "        user_ld.append(ld)\n",
    "        user_loc.append(loc)\n",
    "        user_dst.append(dst)\n",
    "\n",
    "    if user_td: \n",
    "        train_user.append(user)\n",
    "        train_td.append(user_td)\n",
    "        train_ld.append(user_ld)\n",
    "        train_loc.append(user_loc)\n",
    "        train_dst.append(user_dst)\n",
    "\n",
    "    return train_user, train_td, train_ld, train_loc, train_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_user = int(lines[0].split('\\t')[0])\n",
    "for i, line in enumerate(lines):\n",
    "    tokens = line.strip().split('\\t')\n",
    "    user = user2id.get(int(tokens[0]))\n",
    "    # The next line is to get rid of the users\n",
    "    # who have less than 30 records\n",
    "    if user is None:\n",
    "        continue\n",
    "    \n",
    "    # Now, we will only deal with users with more than 30 records.\n",
    "    time = (datetime.strptime(tokens[1], \"%Y-%m-%d\") - datetime(1970, 1, 1)).days\n",
    "    lat, lon, location = tokens[2], tokens[3], tokens[4]\n",
    "    \n",
    "    # The next line creates the poi2id dictionary.\n",
    "    # It maps the existing location id to a new id.\n",
    "    # The new id is defined upon the order of the appearance.\n",
    "    # You can view it as simply rename location id.\n",
    "    if poi2id.get(location) is None:\n",
    "        poi2id[location] = len(poi2id)\n",
    "    loc = poi2id.get(location)\n",
    "    \n",
    "    # When the user is the previous one,\n",
    "    # Just add his attributes into lists accordingly.\n",
    "    # Note that our file is ordered by user id.\n",
    "    # So once a \n",
    "    if user == prev_user:\n",
    "        user_time.insert(0, time)\n",
    "        user_lat.insert(0, lat)\n",
    "        user_lon.insert(0, lon)\n",
    "        user_loc.insert(0, loc)\n",
    "    # We will update train / valid / test lists nnce new user appears.\n",
    "    # To illustrate, each element in train_time is a list containing\n",
    "    # the first 70% time record of a user. Others are similarly defined.\n",
    "    else:\n",
    "        train_threshold = int(len(user_time) * 0.7)\n",
    "        valid_threshold = int(len(user_time) * 0.8)\n",
    "            \n",
    "        train_user.append(user)\n",
    "        train_time.append(user_time[:train_threshold])\n",
    "        train_lat.append(user_lat[:train_threshold])\n",
    "        train_lon.append(user_lon[:train_threshold])\n",
    "        train_loc.append(user_loc[:train_threshold])\n",
    "            \n",
    "        valid_user.append(user)\n",
    "        valid_time.append(user_time[train_threshold:valid_threshold])\n",
    "        valid_lat.append(user_lat[train_threshold:valid_threshold])\n",
    "        valid_lon.append(user_lon[train_threshold:valid_threshold])\n",
    "        valid_loc.append(user_loc[train_threshold:valid_threshold])\n",
    "            \n",
    "        test_user.append(user)\n",
    "        test_time.append(user_time[valid_threshold:])\n",
    "        test_lat.append(user_lat[valid_threshold:])\n",
    "        test_lon.append(user_lon[valid_threshold:])\n",
    "        test_loc.append(user_loc[valid_threshold:])\n",
    "\n",
    "        prev_user = user\n",
    "        user_time = [time]\n",
    "        user_lat = [lat]\n",
    "        user_lon = [lon]\n",
    "        user_loc = [loc]      \n",
    "\n",
    "if user2id.get(user) is not None:\n",
    "    train_threshold = int(len(user_time) * 0.7)\n",
    "    valid_threshold = int(len(user_time) * 0.8)\n",
    "            \n",
    "    train_user.append(user)\n",
    "    train_time.append(user_time[:train_threshold])\n",
    "    train_lat.append(user_lat[:train_threshold])\n",
    "    train_lon.append(user_lon[:train_threshold])\n",
    "    train_loc.append(user_loc[:train_threshold])\n",
    "            \n",
    "    valid_user.append(user)\n",
    "    valid_time.append(user_time[train_threshold:valid_threshold])\n",
    "    valid_lat.append(user_lat[train_threshold:valid_threshold])\n",
    "    valid_lon.append(user_lon[train_threshold:valid_threshold])\n",
    "    valid_loc.append(user_loc[train_threshold:valid_threshold])\n",
    "            \n",
    "    test_user.append(user)\n",
    "    test_time.append(user_time[valid_threshold:])\n",
    "    test_lat.append(user_lat[valid_threshold:])\n",
    "    test_lon.append(user_lon[valid_threshold:])\n",
    "    test_loc.append(user_loc[valid_threshold:])\n",
    "\n",
    "kk = train_time[:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
